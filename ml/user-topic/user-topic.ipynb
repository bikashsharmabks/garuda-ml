{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n",
      "Count for data: 63\n",
      "Train count {features/labels}: 44/44\n",
      "Test count {features/labels}: 19/19\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_89 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 514,564\n",
      "Trainable params: 514,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "44/44 [==============================] - 0s 874us/step - loss: 1.6280 - acc: 0.0909\n",
      "Epoch 2/5\n",
      "44/44 [==============================] - 0s 565us/step - loss: 1.5962 - acc: 0.0909\n",
      "Epoch 3/5\n",
      "44/44 [==============================] - 0s 541us/step - loss: 1.5628 - acc: 0.1136\n",
      "Epoch 4/5\n",
      "44/44 [==============================] - 0s 678us/step - loss: 1.5258 - acc: 0.1136\n",
      "Epoch 5/5\n",
      "44/44 [==============================] - 0s 419us/step - loss: 1.4876 - acc: 0.1364\n",
      "19/19 [==============================] - 0s 417us/step\n",
      "Loss : 1.489090919494629   Accuracy: 0.15789473056793213\n",
      "\n",
      "name: Garrett Mitchell \n",
      "description: Professional hockey player. Married to an amazing wife, Father of two amazing girls. Born and raised in Saskatchewan. \n",
      "predicted class: sports\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, cross_validation\n",
    "import numpy as np\n",
    "\n",
    "data_path = '/data/users-csv.csv'\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "COLUMN_NAMES = ['screen_name','name','description', 'class']\n",
    "\n",
    "# CLASSES = ['sports','technology','entertainment', 'politics',\n",
    "#            'music','legal','medical','education','journalism']\n",
    "\n",
    "CLASSES = ['sports', 'politics', 'education','journalism']\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MAX_VOCABULARY_SIZE = 1000\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5\n",
    "\n",
    "def raw_data_fn(y_name='class'):\n",
    "    data = pd.read_csv(data_path, header=0)\n",
    "    \n",
    "    #drop all df with value NaN\n",
    "    data = data.dropna(subset=['name', 'description', 'class'])\n",
    "    \n",
    "    #count total data\n",
    "    print(\"Count for data: %s\" % len(data))\n",
    "    \n",
    "    #append name and description\n",
    "    name_description = data['name'].astype(str) + ' ' + data['description']   \n",
    "    #print(name_description)\n",
    "    \n",
    "    #get labels \n",
    "    classes = data['class']\n",
    "\n",
    "    # define Tokenizer with Vocab Size\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n",
    "    tokenizer.fit_on_texts(name_description)\n",
    "    \n",
    "    # define encoder and make labels\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(CLASSES)\n",
    "    \n",
    "    #make features and labels\n",
    "    labels = encoder.transform(classes)\n",
    "    features = tokenizer.texts_to_matrix(name_description, mode='tfidf')\n",
    "       \n",
    "    return (features, labels, tokenizer)\n",
    "\n",
    "def load_data_fn(features, labels, test_size=0.3):\n",
    "    return cross_validation.train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(MAX_VOCABULARY_SIZE,)))\n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax))\n",
    "\n",
    "    # Create a TensorFlow optimizer, rather than using the Keras version\n",
    "    # This is currently necessary when working in eager mode\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # We will now compile and print out a summary of our model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def predict_fn(model, labels, tokenizer, name, description):\n",
    "    name_description = name + ' ' + description\n",
    "\n",
    "    feature = tokenizer.texts_to_matrix(np.array([name_description]), mode='tfidf')\n",
    "    prediction = model.predict(feature)\n",
    "    \n",
    "    predicted_label = CLASSES[np.argmax(prediction[0])]\n",
    "    return predicted_label\n",
    "    \n",
    "\n",
    "    \n",
    "features, labels, tokenizer = raw_data_fn()\n",
    "#print(features[0])\n",
    "#print(labels[0])\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = load_data_fn(features, labels)\n",
    "print(\"Train count {features/labels}: %s/%s\" % (len(train_features), len(train_labels)))\n",
    "print(\"Test count {features/labels}: %s/%s\" % (len(test_features), len(test_labels)))\n",
    "\n",
    "# #train_dataset = train_input_fn(train_features, train_labels, BATCH_SIZE)\n",
    "# #print(train_dataset)\n",
    "\n",
    "#create a model for training\n",
    "model = model_fn()\n",
    "\n",
    "#train the training set with model\n",
    "model.fit(train_features, train_labels, epochs=EPOCHS)\n",
    "\n",
    "#evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(\"Loss : %s   Accuracy: %s\" % (loss, accuracy))\n",
    "\n",
    "name = \"Garrett Mitchell\"\n",
    "description = \"Professional hockey player. Married to an amazing wife, Father of two amazing girls. Born and raised in Saskatchewan.\"\n",
    "\n",
    "predicted_label = predict_fn(model, labels, tokenizer, name, description)\n",
    "print(\"\\nname: %s \\ndescription: %s \\npredicted class: %s\" % (name, description, predicted_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
