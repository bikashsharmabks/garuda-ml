{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n",
      "Count for data: 63\n",
      "Train count {features/labels}: 44/44\n",
      "Test count {features/labels}: 19/19\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_185 (Dense)            (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 644,868\n",
      "Trainable params: 644,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "44/44 [==============================] - 0s 810us/step - loss: 1.2884 - acc: 0.2727\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 0s 260us/step - loss: 1.2635 - acc: 0.3409\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 0s 320us/step - loss: 1.2380 - acc: 0.3409\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 0s 354us/step - loss: 1.2121 - acc: 0.3636\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 0s 491us/step - loss: 1.1857 - acc: 0.4545\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 0s 528us/step - loss: 1.1588 - acc: 0.5227\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 0s 335us/step - loss: 1.1316 - acc: 0.5455\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 0s 315us/step - loss: 1.1042 - acc: 0.5909\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 0s 411us/step - loss: 1.0765 - acc: 0.6818\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 0s 594us/step - loss: 1.0486 - acc: 0.6818\n",
      "19/19 [==============================] - 0s 775us/step\n",
      "Loss : 1.2782313823699951   Accuracy: 0.3684210479259491\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "\n",
      "name: Garrett Mitchell \n",
      "description: Professional hockey player. Washington Capitals \n",
      "predicted class: politics\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, cross_validation\n",
    "import numpy as np\n",
    "\n",
    "data_path = '/data/users-csv.csv'\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "COLUMN_NAMES = ['screen_name','name','description', 'class']\n",
    "\n",
    "# CLASSES = ['sports','technology','entertainment', 'politics',\n",
    "#            'music','legal','medical','education','journalism']\n",
    "\n",
    "CLASSES = ['sports', 'politics', 'education','journalism']\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MAX_VOCABULARY_SIZE = 1000\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "    \n",
    "\n",
    "def raw_data_fn(y_name='class'):\n",
    "    data = pd.read_csv(data_path, header=0)\n",
    "    \n",
    "    #drop all df with value NaN\n",
    "    data = data.dropna(subset=['name', 'description', 'class'])\n",
    "    \n",
    "    #count total data\n",
    "    print(\"Count for data: %s\" % len(data))\n",
    "    \n",
    "    #append name and description\n",
    "    name_description = data['name'].astype(str) + ' ' + data['description']   \n",
    "    #print(name_description)\n",
    "    \n",
    "    #get labels \n",
    "    classes = data['class']\n",
    "\n",
    "    # define Tokenizer with Vocab Size\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n",
    "    tokenizer.fit_on_texts(name_description)\n",
    "    \n",
    "    # print the word_index aka what is the equivaluant for each unique word\n",
    "    #print(tokenizer.word_index)\n",
    "    \n",
    "    # define encoder and make labels\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(CLASSES)\n",
    "    \n",
    "    #make features and labels\n",
    "    labels = encoder.transform(classes)\n",
    "    features = tokenizer.texts_to_matrix(name_description, mode='tfidf')\n",
    "       \n",
    "    return (features, labels, tokenizer)\n",
    "\n",
    "def load_data_fn(features, labels, test_size=0.3):\n",
    "    return cross_validation.train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(MAX_VOCABULARY_SIZE,)))\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax))\n",
    "\n",
    "    # Create a TensorFlow optimizer, rather than using the Keras version\n",
    "    # This is currently necessary when working in eager mode\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # We will now compile and print out a summary of our model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def predict_fn(model, labels, tokenizer, name, description):\n",
    "    name_description = name + ' ' + description\n",
    "\n",
    "    feature = tokenizer.texts_to_matrix(np.array([name_description]), mode='tfidf')\n",
    "    prediction = model.predict(feature)\n",
    "    \n",
    "    predicted_label = CLASSES[np.argmax(prediction[0])]\n",
    "    return predicted_label\n",
    "\n",
    "    \n",
    "features, labels, tokenizer = raw_data_fn()\n",
    "#print(features[0])\n",
    "#print(labels[0])\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = load_data_fn(features, labels)\n",
    "print(\"Train count {features/labels}: %s/%s\" % (len(train_features), len(train_labels)))\n",
    "print(\"Test count {features/labels}: %s/%s\" % (len(test_features), len(test_labels)))\n",
    "\n",
    "# #train_dataset = train_input_fn(train_features, train_labels, BATCH_SIZE)\n",
    "# #print(train_dataset)\n",
    "\n",
    "#create a model for training\n",
    "model = model_fn()\n",
    "\n",
    "#create a saver to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#train the training set with model\n",
    "model.fit(train_features, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "#evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(\"Loss : %s   Accuracy: %s\" % (loss, accuracy))\n",
    "\n",
    "\n",
    "model.save(\"user_topic.ckpt\")\n",
    "\n",
    "name = \"Garrett Mitchell\"\n",
    "description = \"Professional hockey player. Washington Capitals\"\n",
    "\n",
    "predicted_label = predict_fn(model, labels, tokenizer, name, description)\n",
    "print(\"\\nname: %s \\ndescription: %s \\npredicted class: %s\" % (name, description, predicted_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
