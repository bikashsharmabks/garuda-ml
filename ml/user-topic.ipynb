{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n",
      "Count for data: 63\n",
      "Train count {features/labels}: 50/50\n",
      "Test count {features/labels}: 13/13\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 514,564\n",
      "Trainable params: 514,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 0s 672us/step - loss: 1.5633 - acc: 0.1000\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 0s 472us/step - loss: 1.5355 - acc: 0.1000\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 0s 589us/step - loss: 1.5058 - acc: 0.1000\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 0s 480us/step - loss: 1.4740 - acc: 0.1000\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 0s 773us/step - loss: 1.4407 - acc: 0.1400\n",
      "13/13 [==============================] - 0s 568us/step\n",
      "1.502903699874878\n",
      "0.1538461595773697\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, cross_validation\n",
    "\n",
    "data_path = '/data/users-csv.csv'\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "COLUMN_NAMES = ['screen_name','name','description', 'class']\n",
    "\n",
    "CLASSES = ['sports','technology','entertainment', 'politics',\n",
    "           'music','legal','medical','education','journalism']\n",
    "\n",
    "NUM_CLASSES = 4 #len(CLASSES)\n",
    "MAX_VOCABULARY_SIZE = 1000\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5\n",
    "\n",
    "def raw_data_fn(y_name='class'):\n",
    "    data = pd.read_csv(data_path, header=0)\n",
    "    \n",
    "    #drop all df with value NaN\n",
    "    data = data.dropna(subset=['name', 'description', 'class'])\n",
    "    \n",
    "    #count total data\n",
    "    print(\"Count for data: %s\" % len(data))\n",
    "    \n",
    "    #append name and description\n",
    "    name_description = data['name'].astype(str) + ' ' + data['description']   \n",
    "   \n",
    "    #get labels \n",
    "    classes = data['class']\n",
    "\n",
    "    # define Tokenizer with Vocab Size\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n",
    "    tokenizer.fit_on_texts(name_description)\n",
    "    \n",
    "    # define encoder and make labels\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(classes)\n",
    "    \n",
    "    #make features and labels\n",
    "    labels = encoder.transform(classes)\n",
    "    features = tokenizer.texts_to_matrix(name_description, mode='tfidf')\n",
    "       \n",
    "    return (features, labels)\n",
    "\n",
    "def load_data_fn(features, labels, test_size=0.2):\n",
    "    return cross_validation.train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "   \n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(MAX_VOCABULARY_SIZE,)))\n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax))\n",
    "\n",
    "    # Create a TensorFlow optimizer, rather than using the Keras version\n",
    "    # This is currently necessary when working in eager mode\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # We will now compile and print out a summary of our model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "    \n",
    "    \n",
    "features, labels = raw_data_fn()\n",
    "#print(features[0])\n",
    "#print(labels[0])\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = load_data_fn(features, labels)\n",
    "print(\"Train count {features/labels}: %s/%s\" % (len(train_features), len(train_labels)))\n",
    "print(\"Test count {features/labels}: %s/%s\" % (len(test_features), len(test_labels)))\n",
    "\n",
    "#train_dataset = train_input_fn(train_features, train_labels, BATCH_SIZE)\n",
    "#print(train_dataset)\n",
    "\n",
    "#create a model for training\n",
    "model = model_fn()\n",
    "\n",
    "#train the training set with model\n",
    "model.fit(train_features, train_labels, epochs=EPOCHS)\n",
    "\n",
    "#evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(loss)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
