{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing, cross_validation\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'resize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-75ee75696748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#return (features, labels, tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mraw_data_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m# features, labels, tokenizer = raw_data_fn()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# #print(features[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-75ee75696748>\u001b[0m in \u001b[0;36mraw_data_fn\u001b[0;34m(y_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# text data preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#shape:16*500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6300000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'resize'"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/card-detection/features/features.csv'\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "COLUMN_NAMES = ['data','hasFace','image_contour', 'class']\n",
    "\n",
    "CLASSES = ['driving license', 'financial card', 'text']\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MAX_VOCABULARY_SIZE = 500\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "def raw_data_fn(y_name='class'):\n",
    "    dataset = pd.read_csv(data_path, encoding='utf-8')\n",
    "    \n",
    "    #drop all df with value NaN\n",
    "    dataset = dataset.dropna(subset=['data', 'hasFace', 'image_contour', 'class'])\n",
    "    TRAINING_SIZE = len(dataset)\n",
    "    #TEST_SIZE = len(dataset)\n",
    "    train_images = np.empty((TRAINING_SIZE,600,1050), np.float32)\n",
    "    \n",
    "    image_path = dataset['image_contour']\n",
    "    i=0\n",
    "    for img in image_path:\n",
    "        x = load_img(img)\n",
    "        x = img_to_array(x)\n",
    "        x = x[:,:,0]\n",
    "        train_images[i,:,:] = x\n",
    "        i+=1\n",
    "        \n",
    "    # Reshape from (N, 28, 28) to (N, 784)\n",
    "    train_images = np.reshape(train_images, (TRAINING_SIZE, 630000))\n",
    "    train_images /= 255  #shape:16*6300000\n",
    "   \n",
    "    # text data preparation \n",
    "    description = dataset['data'].astype(str)    #shape:16*500\n",
    "    description.resize(16, 6300000)\n",
    "    print(description.shape)\n",
    "    \n",
    "    #get labels \n",
    "    classes = dataset['class']\n",
    "\n",
    "    # define Tokenizer with Vocab Size\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n",
    "    tokenizer.fit_on_texts(description)\n",
    "    \n",
    "    # define encoder and make labels\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(CLASSES)\n",
    "    \n",
    "    #make features and labels\n",
    "    labels = encoder.transform(classes)\n",
    "    features = tokenizer.texts_to_matrix(description, mode='tfidf')\n",
    "    #print(features.shape)\n",
    "    \n",
    "    #prepare boolean data\n",
    "    hasFace = dataset[\"hasFace\"]   #shape:16*1\n",
    "    #return (features, labels, tokenizer)\n",
    "    \n",
    "raw_data_fn()\n",
    "# features, labels, tokenizer = raw_data_fn()\n",
    "# #print(features[0])\n",
    "# #print(labels)\n",
    "\n",
    "# def load_data_fn(features, labels, test_size=0.3):\n",
    "#     return cross_validation.train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "# def model_fn():\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(MAX_VOCABULARY_SIZE,)))\n",
    "#     model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax))\n",
    "\n",
    "#     # Create a TensorFlow optimizer, rather than using the Keras version\n",
    "#     # This is currently necessary when working in eager mode\n",
    "#     optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "#     # We will now compile and print out a summary of our model\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=optimizer,\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = load_data_fn(features, labels)\n",
    "# print(\"Train count {features/labels}: %s/%s\" % (len(train_features), len(train_labels)))\n",
    "# print(\"Test count {features/labels}: %s/%s\" % (len(test_features), len(test_labels)))\n",
    "\n",
    "# #create a model for training\n",
    "# model = model_fn()\n",
    "\n",
    "# #train the training set with model\n",
    "# model.fit(train_features, train_labels, epochs=EPOCHS)\n",
    "\n",
    "# #evaluate the model on test set\n",
    "# loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "# print(\"Loss : %s   Accuracy: %s\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gImg = cv2.imread(info['image_contour']) \n",
    "# temp_t = tf.image.convert_image_dtype(\n",
    "#     gImg,\n",
    "#     tf.float32\n",
    "# )\n",
    "\n",
    "def predict_fn(model, labels, tokenizer, description):\n",
    "    name_description = description\n",
    "\n",
    "    feature = tokenizer.texts_to_matrix(np.array([name_description]), mode='tfidf')\n",
    "    prediction = model.predict(feature)\n",
    "    \n",
    "    predicted_label = CLASSES[np.argmax(prediction[0])]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description1 = \"Text messaging, or texting, is the act of composing and sending electronic messages, typically consisting of alphabetic and numeric characters, between two or more users of mobile phones, tablets, desktops/laptops, or other devices.\"\n",
    "description2 = 'LICENCE 1 STANLEY 2 JOE OLIVIA 3 11 03 1976 UNITED KINGDOM 4a  MORGA7531163M9 BURNS CRESCENT EDINBURGH EH1 QGP 9 k l n p q k 7mm rl DVLA INTERNAL USAGE'\n",
    "description3 = 'random gibberish text which is neither a driving license or a financial card 347437493 290422 BHHDS *()&^'\n",
    "\n",
    "predicted_label = predict_fn(model, labels, tokenizer, description2)\n",
    "print(\"description: %s \\npredicted class: %s\" % (description2, predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
